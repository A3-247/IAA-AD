{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1991d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "import scipy.io as scio\n",
    "data={ }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed2c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #导入矩阵处理库\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e62d8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./ADNI_MUL_T1_6_24_2024.csv\",encoding=\"ANSI\")\n",
    "df_array = np.array(df)\n",
    "df_list = df_array.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8620b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tA,tB,tC,tD=0,0,0,0\n",
    "X_2dA = [[0 for i in range(8)]for j in range(29)]\n",
    "Y_2dA =[0 for i in range(29)]\n",
    "X_2dB = [[0 for i in range(8)]for j in range(49)]\n",
    "Y_2dB =[0 for i in range(49)]\n",
    "X_2dC = [[0 for i in range(8)]for j in range(51)]\n",
    "Y_2dC =[0 for i in range(51)]\n",
    "X_2dD = [[0 for i in range(8)]for j in range(32)]\n",
    "Y_2dD =[0 for i in range(32)]\n",
    "for i in range(1,161):\n",
    "    if df_list[i][4]=='1':\n",
    "        X_2dA[tA][0]=np.array(df_list[i][20])\n",
    "        X_2dA[tA][1]=np.array(df_list[i][22])\n",
    "        X_2dA[tA][2]=np.array(df_list[i][24])\n",
    "        X_2dA[tA][3]=np.array(df_list[i][26])\n",
    "        X_2dA[tA][4]=np.array(df_list[i][28])\n",
    "        X_2dA[tA][5]=np.array(df_list[i][29])\n",
    "        X_2dA[tA][6]=np.array(df_list[i][30])\n",
    "        X_2dA[tA][7]=np.array(df_list[i][19])\n",
    "        Y_2dA[tA]=np.array(df_list[i][4])\n",
    "        tA=tA+1\n",
    "for i in range(1,161):\n",
    "    if df_list[i][4]=='2':\n",
    "        X_2dB[tB][0]=np.array(df_list[i][20])\n",
    "        X_2dB[tB][1]=np.array(df_list[i][22])\n",
    "        X_2dB[tB][2]=np.array(df_list[i][24])\n",
    "        X_2dB[tB][3]=np.array(df_list[i][26])\n",
    "        X_2dB[tB][4]=np.array(df_list[i][28])\n",
    "        X_2dB[tB][5]=np.array(df_list[i][29])\n",
    "        X_2dB[tB][6]=np.array(df_list[i][30])\n",
    "        X_2dB[tB][7]=np.array(df_list[i][19])\n",
    "        Y_2dB[tB]=np.array(df_list[i][4])\n",
    "        tB=tB+1\n",
    "for i in range(1,161):\n",
    "    if df_list[i][4]=='3':\n",
    "        X_2dC[tC][0]=np.array(df_list[i][20])\n",
    "        X_2dC[tC][1]=np.array(df_list[i][22])\n",
    "        X_2dC[tC][2]=np.array(df_list[i][24])\n",
    "        X_2dC[tC][3]=np.array(df_list[i][26])\n",
    "        X_2dC[tC][4]=np.array(df_list[i][28])\n",
    "        X_2dC[tC][5]=np.array(df_list[i][29])\n",
    "        X_2dC[tC][6]=np.array(df_list[i][30])\n",
    "        X_2dC[tC][7]=np.array(df_list[i][19])\n",
    "        Y_2dC[tC]=np.array(df_list[i][4])\n",
    "        tC=tC+1\n",
    "for i in range(1,161):\n",
    "    if df_list[i][4]=='4':\n",
    "        X_2dD[tD][0]=np.array(df_list[i][20])\n",
    "        X_2dD[tD][1]=np.array(df_list[i][22])\n",
    "        X_2dD[tD][2]=np.array(df_list[i][24])\n",
    "        X_2dD[tD][3]=np.array(df_list[i][26])\n",
    "        X_2dD[tD][4]=np.array(df_list[i][28])\n",
    "        X_2dD[tD][5]=np.array(df_list[i][29])\n",
    "        X_2dD[tD][6]=np.array(df_list[i][30])\n",
    "        X_2dD[tD][7]=np.array(df_list[i][19])\n",
    "        Y_2dD[tD]=np.array(df_list[i][4])\n",
    "        tD=tD+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f30a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667089f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance Calculation\n",
    "from sklearn.metrics.pairwise import nan_euclidean_distances\n",
    "imputer = KNNImputer(n_neighbors=15)\n",
    "X_2dA=imputer.fit_transform(X_2dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance Calculation\n",
    "from sklearn.metrics.pairwise import nan_euclidean_distances\n",
    "imputer = KNNImputer(n_neighbors=25)\n",
    "X_2dB=imputer.fit_transform(X_2dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4759910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance Calculation\n",
    "from sklearn.metrics.pairwise import nan_euclidean_distances\n",
    "imputer = KNNImputer(n_neighbors=26)\n",
    "X_2dC=imputer.fit_transform(X_2dC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77442e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance Calculation\n",
    "from sklearn.metrics.pairwise import nan_euclidean_distances\n",
    "imputer = KNNImputer(n_neighbors=18)\n",
    "X_2dD=imputer.fit_transform(X_2dD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fMRI\n",
    "I=0\n",
    "for i in range(161):\n",
    "    if i<10:\n",
    "        data[I]=scio.loadmat('./resultsROI_Subject00'+str(i)+'_Condition001.mat')\n",
    "    elif i>=10 and i<=99:\n",
    "        data[I]=scio.loadmat('./resultsROI_Subject0'+str(i)+'_Condition001.mat')\n",
    "    else:\n",
    "        data[I]=scio.loadmat('./resultsROI_Subject'+str(i)+'_Condition001.mat')\n",
    "    I=I+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56382641",
   "metadata": {},
   "outputs": [],
   "source": [
    "XXX_2d = [[0 for i in range(13858)]for j in range(161)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "035b250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X ={ }\n",
    "for i in range(161):\n",
    "    X[i]=data[i]['Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0\n",
    "for i in range(161):\n",
    "    print(i)\n",
    "    t=0\n",
    "    for k in range(0,164):\n",
    "        for l in range(k+1,164):\n",
    " #           print(k,l)\n",
    "            XXX_2d[i][t]=np.array(X[i][k,l])\n",
    "            t=t+1\n",
    "    for k in range(0,164):\n",
    "        for l in range(164,167):\n",
    "            XXX_2d[i][t]=np.array(X[i][k,l])\n",
    "            t=t+1\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64177be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_2d =[0 for i in range(161)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffc799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "It=0\n",
    "for i in range(161):\n",
    "    Y_2d[It]=df_list[i][4]\n",
    "    It=It+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58489c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX=np.array(XXX_2d)\n",
    "YY=np.array(Y_2d,dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "315cde04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression,f_classif\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f050b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tAA,tBB,tCC,tDD=0,0,0,0\n",
    "X_2d = [[0 for i in range(8)]for j in range(161)]\n",
    "Y_2d =[0 for i in range(161)]\n",
    "t=0\n",
    "for i in range(1,161):\n",
    "    if np.array(df_list[i][4])=='0':\n",
    "        continue;\n",
    "    if np.array(df_list[i][4])=='1':\n",
    "        X_2d[t]=np.array(X_2dA[tAA])\n",
    "        Y_2d[t]=np.array(Y_2dA[tAA])\n",
    "        tAA=tAA+1\n",
    "        t=t+1\n",
    "    if np.array(df_list[i][4])=='2':\n",
    "        X_2d[t]=np.array(X_2dB[tBB])\n",
    "        Y_2d[t]=np.array(Y_2dB[tBB])\n",
    "        tBB=tBB+1\n",
    "        t=t+1\n",
    "    if np.array(df_list[i][4])=='3':\n",
    "        X_2d[t]=np.array(X_2dC[tCC])\n",
    "        Y_2d[t]=np.array(Y_2dC[tCC])\n",
    "        tCC=tCC+1\n",
    "        t=t+1\n",
    "    if np.array(df_list[i][4])=='4':\n",
    "        X_2d[t]=np.array(X_2dD[tDD])\n",
    "        Y_2d[t]=np.array(Y_2dD[tDD])\n",
    "        tDD=tDD+1\n",
    "        t=t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de8d106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d05eca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "573993cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    " \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim_q, dim_k, dim_v):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.dim_q = dim_q\n",
    "        self.dim_k = dim_k\n",
    "        self.dim_v = dim_v\n",
    " \n",
    "        #定义线性变换函数\n",
    "        self.linear_q = nn.Linear(dim_q, dim_k,bias=False)\n",
    "        self.linear_k = nn.Linear(dim_q, dim_k,bias=False)\n",
    "        self.linear_v = nn.Linear(dim_q, dim_v,bias=False)\n",
    "        self._norm_fact = 1 / sqrt(dim_k)\n",
    " \n",
    "    def forward(self, x):\n",
    "        # x: batch, n, dim_q\n",
    " \n",
    "        batch, dim_q = x.shape\n",
    "        #print('batch=',batch,'dim_q',dim_q)\n",
    "        n = 1\n",
    "        assert dim_q == self.dim_q\n",
    "        q = self.linear_q(x)  # batch, n, dim_k\n",
    "        k = self.linear_k(x)  # batch, n, dim_k\n",
    "        v = self.linear_v(x)  # batch, n, dim_v\n",
    "       # print(q.shape)\n",
    "       # print(k.shape)\n",
    "        dist = torch.mm(q, k.transpose(0,1)) * self._norm_fact  # batch, n, n\n",
    "        dist = torch.softmax(dist, dim=-1)  # batch, n, n\n",
    "        att = torch.mm(dist, v)\n",
    "        return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9954e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC=np.array([0 for i in range(1000)],dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71414206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dfAGE = pd.read_csv(\"./R_loss.csv\",encoding=\"UTF-8\")\n",
    "dfAGE_array = np.array(dfAGE)\n",
    "dfAGE_list = dfAGE_array.tolist()\n",
    "print(dfAGE_array[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3833d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67a02810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e45b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "ALL_best=0\n",
    "Best_ACC=0\n",
    "Best_RE=0\n",
    "Best_F1=0\n",
    "Best_FLAG=0\n",
    "LOW_ACC=1.1\n",
    "LOW_RE=1.1\n",
    "LOW_F1=1.1\n",
    "LOW_FLAG=0\n",
    "AVG_ACC=0\n",
    "DF=0;\n",
    "DB=0;\n",
    "DV=0;\n",
    "DM=0;\n",
    "op_now=0\n",
    "ACCNOW=[0 for i in range(825)]\n",
    "for iii in range(50,750,1):\n",
    "    selector = SelectKBest(f_classif, k=iii)\n",
    "    X_train =np.array([[0 for i in range(13858)] for j in range(138)],dtype='float')\n",
    "    X_test =np.array([[0 for i in range(13858)] for j in range(23)],dtype='float')\n",
    "    Y_train=np.array([0 for i in range(138)],dtype='int')\n",
    "    Y_test=np.array([0 for i in range(23)],dtype='int')\n",
    "    X_trainA =np.array([[0 for i in range(iii+13)] for j in range(138)],dtype='float')\n",
    "    X_testA =np.array([[0 for i in range(iii+13)] for j in range(23)],dtype='float')\n",
    "    Y_trainA=np.array([0 for i in range(138)],dtype='int')\n",
    "    Y_testA=np.array([0 for i in range(23)],dtype='int')\n",
    "    u,v=0,0;\n",
    "    for j in range(161):\n",
    "        if (j%7==op_now):\n",
    "            X_test[u]=XX[j]\n",
    "            Y_test[u]=YY[j]\n",
    "            u=u+1;\n",
    "        else:\n",
    "            X_train[v]=XX[j]\n",
    "            Y_train[v]=YY[j]\n",
    "            v=v+1;\n",
    "    for kkk in range(60):\n",
    "        X_newR = selector.fit_transform(X_train,Y_train)\n",
    "        X_newE = selector.transform(X_test)\n",
    "        model=SelfAttention(161,iii+13,iii+13)\n",
    "        X_newRA = np.array([[0 for i in range(iii+13)] for j in range(138)],dtype='float')\n",
    "        X_newEA = np.array([[0 for i in range(iii+13)] for j in range(23)],dtype='float')\n",
    "        Feature=np.zeros((iii+13,161))\n",
    "        u,v=0,0;\n",
    "        for i in range(161):\n",
    "            if (i%7==op_now):\n",
    "                for j in range(iii):\n",
    "                    X_newEA[u][j]=X_newE[u][j]\n",
    "                t=iii\n",
    "                for j in range(7):\n",
    "                    X_newEA[u][t]=np.array(X_2d[i][j],dtype=float)\n",
    "                    t=t+1\n",
    "                for j in range(6):\n",
    "                    X_newEA[u][t]=np.array(dfAGE_list[i][j],dtype=float)\n",
    "                    t=t+1\n",
    "                u=u+1\n",
    "            else:\n",
    "                for j in range(iii):\n",
    "                    X_newRA[v][j]=X_newR[v][j]\n",
    "                t=iii\n",
    "                for j in range(7):\n",
    "                    X_newRA[v][t]=np.array(X_2d[i][j],dtype=float)\n",
    "                    t=t+1\n",
    "                for j in range(6):\n",
    "                    X_newRA[v][t]=np.array(dfAGE_list[i][j],dtype=float)\n",
    "                    t=t+1\n",
    "                v=v+1\n",
    "        for i in range(iii+13):\n",
    "            u,v=0,0;\n",
    "            for j in range(161):\n",
    "                if (j%7==op_now):\n",
    "                    Feature[i][j]=np.array(X_newEA[u][i])\n",
    "                    u=u+1\n",
    "                else:\n",
    "                    Feature[i][j]=np.array(X_newRA[v][i])\n",
    "                    v=v+1\n",
    "        Feature_tensor = torch.from_numpy(Feature)\n",
    "#    print('Feature_shape:',Feature_tensor.shape)\n",
    "        YQ1=model(Feature_tensor)\n",
    "#         tagAA=[0 for i in range(iii+13)]\n",
    "#         for i in range(iii+13):\n",
    "#             tagAA[i]=sum(YQ1[i]).detach().numpy()\n",
    "        tagAA=torch.sum(YQ1, dim=1)\n",
    "        u,v=0,0;\n",
    "        for i in range(161):\n",
    "            if (i%7==op_now):\n",
    "                for j in range(iii+13):\n",
    "                    X_testA[u][j]=X_newEA[u][j]*tagAA[j].detach().numpy()\n",
    "                u=u+1\n",
    "            else:\n",
    "                for j in range(iii+13):\n",
    "                    X_trainA[v][j]=X_newRA[v][j]*tagAA[j].detach().numpy()\n",
    "                v=v+1\n",
    "        ACC=0;\n",
    "        cnt=0;\n",
    "        clf_linear = svm.SVC(kernel='linear',C=0.095)\n",
    "        clf_linear.fit(X_trainA,Y_train)\n",
    "        score_linear_test = clf_linear.score(X_testA,Y_test)\n",
    "        score_linear_train = clf_linear.score(X_trainA,Y_train)\n",
    "        predict_test = clf_linear.predict(X_testA)\n",
    "#         print(\"SVM Test  Accuracy : %.4g\" % (score_linear_test))\n",
    "#         print(\"SVM Train  Accuracy : %.4g\" % (score_linear_train))\n",
    "        ACC=ACC+score_linear_test;\n",
    "        cnt=cnt+1;\n",
    "        ACC=ACC/cnt\n",
    "        AVG_ACC=AVG_ACC+ACC\n",
    "        if ACC>Best_ACC:\n",
    "            Best_FLAG=iii\n",
    "            Best_ACC=ACC\n",
    "        if ACC<LOW_ACC:\n",
    "            LOW_FLAG=iii\n",
    "            LOW_ACC=ACC\n",
    "        print(iii,kkk,ACC)\n",
    "        if (ACCNOW[iii]<ACC):\n",
    "            ACCNOW[iii]=ACC\n",
    "            PATH = \"./ARZE\"+str(iii)+\"_P\"+str(op_now)+\".pth\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            savemat('X_now_train'+str(iii)+'_P'+str(op_now)+'.mat', {'data': X_newRA})\n",
    "            savemat('X_now_test'+str(iii)+'_P'+str(op_now)+'.mat', {'data': X_newEA})\n",
    "    print(iii,ACCNOW[iii])\n",
    "print(\"BEST:\")\n",
    "print(Best_FLAG,Best_ACC)\n",
    "print(LOW_FLAG,LOW_ACC)\n",
    "print(\"ACC\")\n",
    "\n",
    "print(\"  \")\n",
    "print(\"  \")\n",
    "print(\"   \")\n",
    "\n",
    "for i in range(50,750):\n",
    "    print(ACCNOW[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22bbf003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 0.782608695652174\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "op_now=0\n",
    "ACCNOW=[0 for i in range(825)]\n",
    "iii=53\n",
    "selector = SelectKBest(f_classif, k=iii)\n",
    "X_train =np.array([[0 for i in range(13858)] for j in range(138)],dtype='float')\n",
    "X_test =np.array([[0 for i in range(13858)] for j in range(23)],dtype='float')\n",
    "Y_train=np.array([0 for i in range(138)],dtype='int')\n",
    "Y_test=np.array([0 for i in range(23)],dtype='int')\n",
    "X_trainA =np.array([[0 for i in range(iii+13)] for j in range(138)],dtype='float')\n",
    "X_testA =np.array([[0 for i in range(iii+13)] for j in range(23)],dtype='float')\n",
    "Y_trainA=np.array([0 for i in range(138)],dtype='int')\n",
    "Y_testA=np.array([0 for i in range(23)],dtype='int')\n",
    "u,v=0,0;\n",
    "for j in range(161):\n",
    "    if (j%7==op_now):\n",
    "        X_test[u]=XX[j]\n",
    "        Y_test[u]=YY[j]\n",
    "        u=u+1;\n",
    "    else:\n",
    "        X_train[v]=XX[j]\n",
    "        Y_train[v]=YY[j]\n",
    "        v=v+1;\n",
    "X_newR = selector.fit_transform(X_train,Y_train)\n",
    "X_newE = selector.transform(X_test)\n",
    "model=SelfAttention(161,iii+13,iii+13)\n",
    "save_model = torch.load(\"./ARZE\"+str(iii)+\"_P\"+str(op_now)+\".pth\")\n",
    "model_dict =  model.state_dict()\n",
    "state_dict = {k:v for k,v in save_model.items() if k in model_dict.keys()}\n",
    "model_dict.update(state_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "X_newRA = np.array([[0 for i in range(iii+13)] for j in range(138)],dtype='float')\n",
    "X_newEA = np.array([[0 for i in range(iii+13)] for j in range(23)],dtype='float')\n",
    "Feature=np.zeros((iii+13,161))\n",
    "u,v=0,0;\n",
    "for i in range(161):\n",
    "    if (i%7==op_now):\n",
    "        for j in range(iii):\n",
    "            X_newEA[u][j]=X_newE[u][j]\n",
    "        t=iii\n",
    "        for j in range(7):\n",
    "            X_newEA[u][t]=np.array(X_2d[i][j],dtype=float)\n",
    "            t=t+1\n",
    "        for j in range(6):\n",
    "            X_newEA[u][t]=np.array(dfAGE_list[i][j],dtype=float)\n",
    "            t=t+1\n",
    "        u=u+1\n",
    "    else:\n",
    "        for j in range(iii):\n",
    "            X_newRA[v][j]=X_newR[v][j]\n",
    "        t=iii\n",
    "        for j in range(7):\n",
    "            X_newRA[v][t]=np.array(X_2d[i][j],dtype=float)\n",
    "            t=t+1\n",
    "        for j in range(6):\n",
    "            X_newRA[v][t]=np.array(dfAGE_list[i][j],dtype=float)\n",
    "            t=t+1\n",
    "        v=v+1\n",
    "for i in range(iii+13):\n",
    "    u,v=0,0;\n",
    "    for j in range(161):\n",
    "        if (j%7==op_now):\n",
    "            Feature[i][j]=np.array(X_newEA[u][i])\n",
    "            u=u+1\n",
    "        else:\n",
    "            Feature[i][j]=np.array(X_newRA[v][i])\n",
    "            v=v+1\n",
    "Feature_tensor = torch.from_numpy(Feature)\n",
    "#    print('Feature_shape:',Feature_tensor.shape)\n",
    "YQ1=model(Feature_tensor)\n",
    "#         tagAA=[0 for i in range(iii+13)]\n",
    "#         for i in range(iii+13):\n",
    "#             tagAA[i]=sum(YQ1[i]).detach().numpy()\n",
    "tagAA=torch.sum(YQ1, dim=1)\n",
    "u,v=0,0;\n",
    "for i in range(161):\n",
    "    if (i%7==op_now):\n",
    "        for j in range(iii+13):\n",
    "            X_testA[u][j]=X_newEA[u][j]*tagAA[j].detach().numpy()\n",
    "        u=u+1\n",
    "    else:\n",
    "        for j in range(iii+13):\n",
    "            X_trainA[v][j]=X_newRA[v][j]*tagAA[j].detach().numpy()\n",
    "        v=v+1\n",
    "#         X_testA = scaler.fit_transform(X_testA)\n",
    "#         X_trainA = scaler.fit_transform(X_trainA)\n",
    "ACC=0;\n",
    "cnt=0;\n",
    "clf_linear = svm.SVC(kernel='linear',C=0.095)\n",
    "clf_linear.fit(X_trainA,Y_train)\n",
    "score_linear_test = clf_linear.score(X_testA,Y_test)\n",
    "score_linear_train = clf_linear.score(X_trainA,Y_train)\n",
    "predict_test = clf_linear.predict(X_testA)\n",
    "#         print(\"SVM Test  Accuracy : %.4g\" % (score_linear_test))\n",
    "#         print(\"SVM Train  Accuracy : %.4g\" % (score_linear_train))\n",
    "ACC=ACC+score_linear_test;\n",
    "cnt=cnt+1;\n",
    "ACC=ACC/cnt\n",
    "print(iii,ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34678e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d27f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "PRED = [0 for i in range(161)]\n",
    "SOFTMAX = []\n",
    "Best_ACC=0\n",
    "Best_RE=0\n",
    "Best_F1=0\n",
    "Best_FLAG=0\n",
    "LOW_ACC=1.1\n",
    "LOW_RE=1.1\n",
    "LOW_F1=1.1\n",
    "LOW_FLAG=0\n",
    "AVG_ACC=0\n",
    "AC_F=0;\n",
    "AC_B=0;\n",
    "AC_V=0;\n",
    "AC_M=0;\n",
    "all_F=0;\n",
    "all_B=0;\n",
    "all_V=0;\n",
    "all_M=0;\n",
    "DF=0;\n",
    "DB=0;\n",
    "DV=0;\n",
    "DM=0;\n",
    "selector = SelectKBest(f_regression, k=194)\n",
    "X_new = selector.fit_transform(XX,YY)\n",
    "X_newA = [[0 for i in range(207)] for j in range(161)]\n",
    "model=SelfAttention(161,207,207)\n",
    "save_model = torch.load(\"./ARZE194.pth\")\n",
    "model_dict =  model.state_dict()\n",
    "\n",
    "state_dict = {k:v for k,v in save_model.items() if k in model_dict.keys()}\n",
    "print(state_dict.keys())\n",
    "model_dict.update(state_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "print(\"!!!\")\n",
    "Feature=np.zeros((207,161))\n",
    "for i in range(161):\n",
    "    for j in range(194):\n",
    "        X_newA[i][j]=X_new[i][j]\n",
    "for i in range(161):\n",
    "    t=194\n",
    "    for j in range(7):\n",
    "        X_newA[i][t]=np.array(X_2d[i][j],dtype=float)\n",
    "        t=t+1\n",
    "    for j in range(6):\n",
    "        X_newA[i][t]=np.array(dfAGE_list[i][j],dtype=float)\n",
    "        t=t+1\n",
    "print(\"CSS1:\",t)\n",
    "for i in range(207):\n",
    "    for j in range(161):\n",
    "        Feature[i][j]=np.array(X_newA[j][i])\n",
    "Feature_tensor = torch.from_numpy(Feature)\n",
    "print(\"CSS2:\")\n",
    "YQ1=model(Feature_tensor)\n",
    "print(\"CSS3:\")\n",
    "tagAA=[0 for i in range(207)]\n",
    "for i in range(207):\n",
    "    tagAA[i]=sum(YQ1[i]).detach().numpy()\n",
    "# tagAA[5]=tagAA[5]*2\n",
    "# tagAA[47]=tagAA[47]*1.75\n",
    "# for i in range(194,201):\n",
    "#     tagAA[i]=tagAA[i]*1.2\n",
    "for i in range(207):\n",
    "    print(tagAA[i])\n",
    "X_newAtt = [[0 for i in range(207)]for j in range(161)]\n",
    "for i in range(161):\n",
    "    for j in range(207):\n",
    "        X_newAtt[i][j]=X_newA[i][j]*tagAA[j]\n",
    "X_train =[[0 for i in range(207)] for j in range(138)]\n",
    "X_test =[[0 for i in range(207)] for j in range(23)]\n",
    "Y_train=[0 for i in range(138)]\n",
    "Y_test=[0 for i in range(23)]\n",
    "ACC=0;\n",
    "cnt=0;\n",
    "for i in range(7):\n",
    "    u=0;\n",
    "    v=0;\n",
    "    for j in range(161):\n",
    "        if (j%7==i):\n",
    "            X_test[u]=X_newAtt[j]\n",
    "            Y_test[u]=YY[j]\n",
    "            if Y_test[u]==1:\n",
    "                DF=DF+1;\n",
    "            if Y_test[u]==2:\n",
    "                DB=DB+1;\n",
    "            if Y_test[u]==3:\n",
    "                DV=DV+1;\n",
    "            if Y_test[u]==4:\n",
    "                DM=DM+1;\n",
    "            u=u+1;\n",
    "        else:\n",
    "            X_train[v]=X_newAtt[j]\n",
    "            Y_train[v]=YY[j]\n",
    "            v=v+1;\n",
    "    clf_linear = svm.SVC(kernel='linear',C=0.095,probability = True)\n",
    "    clf_linear.fit(X_train,Y_train)\n",
    "    score_linear_test = clf_linear.score(X_test,Y_test)\n",
    "    score_linear_train = clf_linear.score(X_train,Y_train)\n",
    "    predict_test = clf_linear.predict(X_test)\n",
    "    print(\"SVM Test  Accuracy : %.4g\" % (score_linear_test))\n",
    "    print(\"SVM Train  Accuracy : %.4g\" % (score_linear_train))\n",
    "    ACC=ACC+score_linear_test;\n",
    "    cnt=cnt+1;\n",
    "    print(predict_test)\n",
    "    for iii in range(len(predict_test)):\n",
    "        PRED[iii*7+i]=predict_test[iii]\n",
    "    for ii in range(len(Y_test)):\n",
    "       # print(YY[ii],predict_test);\n",
    "        if predict_test[ii] == 1:\n",
    "            all_F=all_F+1;\n",
    "            if Y_test[ii]==1:\n",
    "                AC_F=AC_F+1\n",
    "        if predict_test[ii] == 2:\n",
    "            all_B=all_B+1;\n",
    "            if Y_test[ii]==2:\n",
    "                AC_B=AC_B+1\n",
    "        if predict_test[ii] == 3: \n",
    "            all_V=all_V+1;\n",
    "            if Y_test[ii]==3:\n",
    "                AC_V=AC_V+1\n",
    "        if predict_test[ii] == 4:\n",
    "            all_M=all_M+1;\n",
    "            if Y_test[ii]==4:\n",
    "                AC_M=AC_M+1\n",
    "    SOFTMAX.append(clf_linear.predict_proba(X_test))\n",
    "ACC=ACC/cnt\n",
    "all_F=AC_F/all_F;\n",
    "all_B=AC_B/all_B;\n",
    "all_V=AC_V/all_V;\n",
    "all_M=AC_M/all_M;\n",
    "AC_F=AC_F/DF\n",
    "AC_B=AC_B/DB\n",
    "AC_V=AC_V/DV\n",
    "AC_M=AC_M/DM\n",
    "F_F=(2*all_F*AC_F)/(AC_F+all_F)\n",
    "F_B=(2*all_B*AC_B)/(AC_B+all_B)\n",
    "F_V=(2*all_V*AC_V)/(AC_V+all_V)\n",
    "F_M=(2*all_M*AC_M)/(AC_M+all_M)\n",
    "print(ACC)\n",
    "print('召回率')\n",
    "print(AC_F)\n",
    "print(AC_B)\n",
    "print(AC_V)\n",
    "print(AC_M)\n",
    "print('F1分数')\n",
    "print(F_F)\n",
    "print(F_B)\n",
    "print(F_V)\n",
    "print(F_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01c97f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hot= np.array([[0 for i in range(4)] for j in range(161)],dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5aa889ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    for j in range(23):\n",
    "        for k in range(4):\n",
    "            X_hot[j*7+i][k]=SOFTMAX[i][j][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0c67755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [6.97791351e-04 9.19092650e-01 7.29365447e-02 7.27301409e-03] 1 1.0 1.0\n",
      "1 [0.00570692 0.60720001 0.29078891 0.09630416] 1 1.0 1.0\n",
      "2 [0.00932277 0.81859052 0.15354638 0.01854034] 1 1.0 1.0\n",
      "3 [0.00660746 0.76290927 0.20203032 0.02845294] 1 1.0 1.0\n",
      "4 [0.00302196 0.86296387 0.08844544 0.04556873] 1 2.0 1.0\n",
      "5 [0.02110509 0.73913693 0.2194495  0.02030848] 1 2.0 1.0\n",
      "6 [0.00619037 0.60633324 0.17708385 0.21039254] 1 2.0 1.0\n",
      "7 [0.06557693 0.02731658 0.45290491 0.45420158] 3 3.0 3.0\n",
      "8 [0.00091239 0.77677264 0.21286963 0.00944534] 1 1.0 1.0\n",
      "9 [0.0070425  0.16677523 0.24733707 0.5788452 ] 3 3.0 3.0\n",
      "10 [0.06998608 0.50582207 0.37038959 0.05380225] 1 1.0 1.0\n",
      "11 [0.00134074 0.04744073 0.60683964 0.34437889] 2 3.0 2.0\n",
      "12 [0.04965287 0.29121431 0.59476082 0.064372  ] 2 2.0 2.0\n",
      "13 [0.01416542 0.45843466 0.31257806 0.21482186] 1 3.0 1.0\n",
      "14 [0.01270188 0.72225419 0.24341047 0.02163346] 1 1.0 1.0\n",
      "15 [0.0033865  0.86645244 0.10843196 0.0217291 ] 1 1.0 1.0\n",
      "16 [0.01522866 0.17190174 0.31468364 0.49818596] 3 2.0 3.0\n",
      "17 [0.01131433 0.33118011 0.50038716 0.1571184 ] 2 2.0 2.0\n",
      "18 [0.05964662 0.01660513 0.49365996 0.43008829] 2 3.0 3.0\n",
      "18 ERROR!!!\n",
      "19 [0.02234267 0.09199814 0.31377336 0.57188583] 3 3.0 3.0\n",
      "20 [0.02536869 0.72295221 0.17617893 0.07550017] 1 1.0 1.0\n",
      "21 [0.02141411 0.51084275 0.30129072 0.16645242] 1 1.0 1.0\n",
      "22 [0.94321149 0.02109429 0.02589738 0.00979683] 0 0.0 0.0\n",
      "23 [0.34565771 0.02977829 0.36627054 0.25829346] 2 0.0 0.0\n",
      "23 ERROR!!!\n",
      "24 [0.00593704 0.02051623 0.43211349 0.54143323] 3 3.0 3.0\n",
      "25 [0.0069643  0.95612772 0.03072709 0.00618089] 1 1.0 1.0\n",
      "26 [0.04874127 0.02112023 0.63255606 0.29758244] 2 3.0 2.0\n",
      "27 [0.00583165 0.1024141  0.78284853 0.10890572] 2 1.0 2.0\n",
      "28 [0.00509125 0.51282448 0.43549972 0.04658456] 1 1.0 1.0\n",
      "29 [0.15280011 0.01984044 0.27829695 0.5490625 ] 3 3.0 3.0\n",
      "30 [0.90908395 0.04263452 0.03503318 0.01324834] 0 0.0 0.0\n",
      "31 [0.03055634 0.29619174 0.56428773 0.10896418] 2 2.0 2.0\n",
      "32 [0.00280848 0.58621674 0.32444179 0.08653299] 1 1.0 1.0\n",
      "33 [0.06047478 0.03962502 0.87564163 0.02425857] 2 3.0 2.0\n",
      "34 [0.03170396 0.02792483 0.76969618 0.17067503] 2 2.0 2.0\n",
      "35 [0.06822366 0.03684553 0.46598827 0.42894254] 2 2.0 2.0\n",
      "36 [0.00289076 0.55748048 0.30446135 0.13516741] 1 1.0 1.0\n",
      "37 [0.00082462 0.68610822 0.27041304 0.04265412] 1 1.0 1.0\n",
      "38 [0.02618197 0.18146615 0.69203706 0.10031482] 2 2.0 2.0\n",
      "39 [0.01409946 0.05290054 0.62657205 0.30642795] 2 2.0 2.0\n",
      "40 [0.01145883 0.59169756 0.38665854 0.01018506] 1 2.0 1.0\n",
      "41 [0.00673399 0.29594669 0.32606212 0.3712572 ] 3 3.0 3.0\n",
      "42 [0.01048786 0.62322456 0.32849126 0.03779631] 1 1.0 1.0\n",
      "43 [0.25906396 0.05873666 0.29409152 0.38810786] 3 3.0 3.0\n",
      "44 [0.05418864 0.0783314  0.34116254 0.52631743] 3 2.0 3.0\n",
      "45 [0.05214032 0.15804015 0.5266201  0.26319943] 2 2.0 2.0\n",
      "46 [0.04828677 0.04721174 0.58817517 0.31632632] 2 2.0 2.0\n",
      "47 [0.01460511 0.1226185  0.7131259  0.1496505 ] 2 2.0 2.0\n",
      "48 [0.00787812 0.54203412 0.41610157 0.03398619] 1 1.0 1.0\n",
      "49 [0.01034887 0.40309509 0.29699143 0.28956462] 1 1.0 1.0\n",
      "50 [0.00406481 0.46152304 0.37527985 0.1591323 ] 1 1.0 1.0\n",
      "51 [0.00230443 0.93510793 0.05448699 0.00810065] 1 1.0 1.0\n",
      "52 [0.04437773 0.42975772 0.40616564 0.1196989 ] 1 1.0 1.0\n",
      "53 [0.02710252 0.28440299 0.45833117 0.23016332] 2 2.0 2.0\n",
      "54 [0.97047776 0.00163221 0.02061651 0.00727352] 0 0.0 0.0\n",
      "55 [0.95365184 0.01987481 0.01127168 0.01520167] 0 0.0 0.0\n",
      "56 [0.01317611 0.14056147 0.4373003  0.40896212] 2 2.0 2.0\n",
      "57 [0.05546853 0.03019985 0.75458463 0.15974698] 2 3.0 2.0\n",
      "58 [0.01077139 0.14212708 0.33323317 0.51386835] 3 1.0 3.0\n",
      "59 [0.94978611 0.00900404 0.0320859  0.00912394] 0 0.0 0.0\n",
      "60 [0.00651619 0.40770659 0.29087791 0.29489931] 1 3.0 1.0\n",
      "61 [0.64075681 0.05108419 0.21469571 0.0934633 ] 0 0.0 0.0\n",
      "62 [0.00394224 0.84595603 0.1158284  0.03427333] 1 2.0 1.0\n",
      "63 [0.00509267 0.24480535 0.58807999 0.16202199] 2 2.0 2.0\n",
      "64 [0.00478816 0.28580053 0.49515522 0.21425609] 2 2.0 2.0\n",
      "65 [0.0125148  0.14795028 0.4632342  0.37630073] 2 2.0 2.0\n",
      "66 [0.00343623 0.8157819  0.09194436 0.08883751] 1 1.0 1.0\n",
      "67 [0.84624207 0.01325967 0.07058151 0.06991676] 0 0.0 0.0\n",
      "68 [0.02802022 0.20381183 0.48830407 0.27986388] 2 3.0 2.0\n",
      "69 [0.00348196 0.68509539 0.2456223  0.06580035] 1 1.0 1.0\n",
      "70 [0.02712881 0.00300264 0.23772293 0.73214562] 3 3.0 3.0\n",
      "71 [0.03933066 0.13798379 0.61137071 0.21131484] 2 2.0 2.0\n",
      "72 [0.00054369 0.01562647 0.48140466 0.50242517] 3 3.0 3.0\n",
      "73 [0.01088143 0.08993464 0.36935279 0.52983113] 3 3.0 3.0\n",
      "74 [0.06779668 0.46914212 0.37714581 0.08591539] 1 1.0 1.0\n",
      "75 [0.30652895 0.40634233 0.17038521 0.11674351] 1 1.0 1.0\n",
      "76 [0.00241996 0.52907836 0.36773733 0.10076434] 1 2.0 1.0\n",
      "77 [0.00141303 0.65067835 0.27881557 0.06909305] 1 1.0 1.0\n",
      "78 [0.00211071 0.23235412 0.6074752  0.15805998] 2 3.0 2.0\n",
      "79 [0.0007596  0.0989312  0.66338474 0.23692446] 2 2.0 2.0\n",
      "80 [0.01429024 0.19198963 0.53565739 0.25806274] 2 2.0 2.0\n",
      "81 [0.18142153 0.46911101 0.29418687 0.05528058] 1 2.0 1.0\n",
      "82 [7.55217676e-04 7.60204962e-01 2.37349033e-01 1.69078788e-03] 1 1.0 1.0\n",
      "83 [0.01041715 0.57425007 0.32737224 0.08796054] 1 1.0 1.0\n",
      "84 [0.05351211 0.29873137 0.58082395 0.06693258] 2 1.0 2.0\n",
      "85 [0.02248797 0.20094375 0.63653059 0.14003769] 2 2.0 2.0\n",
      "86 [0.01638381 0.88991825 0.06786867 0.02582927] 1 2.0 1.0\n",
      "87 [0.0154909  0.0256075  0.33703357 0.62186803] 3 3.0 3.0\n",
      "88 [0.02826208 0.48238442 0.36987327 0.11948024] 1 1.0 1.0\n",
      "89 [0.01455391 0.52923353 0.43129097 0.02492159] 1 1.0 1.0\n",
      "90 [0.00183941 0.75626439 0.21971391 0.02218229] 1 1.0 1.0\n",
      "91 [0.00411772 0.17654038 0.76264344 0.05669846] 2 1.0 2.0\n",
      "92 [0.05838122 0.15265137 0.53513269 0.25383472] 2 2.0 2.0\n",
      "93 [0.03057488 0.04856909 0.73425646 0.18659957] 2 2.0 2.0\n",
      "94 [0.04325588 0.04538303 0.36771274 0.54364835] 3 2.0 3.0\n",
      "95 [0.14547319 0.0062854  0.36362496 0.48461646] 3 3.0 3.0\n",
      "96 [0.19219361 0.09905101 0.48809567 0.22065971] 2 3.0 2.0\n",
      "97 [0.10313919 0.45255733 0.36916531 0.07513816] 1 1.0 1.0\n",
      "98 [0.02567596 0.39151781 0.25229901 0.33050722] 1 1.0 1.0\n",
      "99 [0.0180463  0.49590354 0.38230869 0.10374148] 1 2.0 1.0\n",
      "100 [0.03315175 0.0226708  0.64285591 0.30132154] 2 2.0 2.0\n",
      "101 [0.01214099 0.68609921 0.22155155 0.08020825] 1 2.0 1.0\n",
      "102 [0.00539187 0.06523492 0.61375438 0.31561883] 2 2.0 2.0\n",
      "103 [0.1864967  0.03639056 0.38114848 0.39596426] 3 3.0 3.0\n",
      "104 [0.67025751 0.08921854 0.18316586 0.05735809] 0 0.0 0.0\n",
      "105 [0.00985404 0.57662614 0.26219325 0.15132657] 1 3.0 1.0\n",
      "106 [0.50737609 0.43384998 0.0534446  0.00532934] 0 0.0 0.0\n",
      "107 [0.57178433 0.06022366 0.24625705 0.12173496] 0 0.0 0.0\n",
      "108 [0.0188082  0.73300374 0.21716318 0.03102488] 1 1.0 1.0\n",
      "109 [0.08216525 0.00674068 0.49594178 0.41515229] 2 3.0 3.0\n",
      "109 ERROR!!!\n",
      "110 [0.04549456 0.68372859 0.26150398 0.00927287] 1 1.0 1.0\n",
      "111 [0.06426718 0.18743243 0.15720248 0.59109791] 3 3.0 3.0\n",
      "112 [0.0276712  0.41840443 0.45516315 0.09876123] 2 1.0 1.0\n",
      "112 ERROR!!!\n",
      "113 [0.01031186 0.19837675 0.68440301 0.10690837] 2 2.0 2.0\n",
      "114 [0.01864559 0.79928406 0.17536876 0.00670159] 1 1.0 1.0\n",
      "115 [0.02733063 0.06847152 0.4714616  0.43273625] 2 2.0 3.0\n",
      "115 ERROR!!!\n",
      "116 [0.04244846 0.71395225 0.17205786 0.07154143] 1 0.0 1.0\n",
      "117 [0.68820716 0.03505048 0.24887705 0.02786532] 0 0.0 0.0\n",
      "118 [0.00785251 0.17013675 0.19010669 0.63190405] 3 3.0 3.0\n",
      "119 [0.02597421 0.41789625 0.33191184 0.2242177 ] 1 1.0 1.0\n",
      "120 [0.04323063 0.20682299 0.41904743 0.33089896] 2 2.0 3.0\n",
      "120 ERROR!!!\n",
      "121 [0.03184743 0.22518278 0.47964635 0.26332344] 2 2.0 2.0\n",
      "122 [0.23143643 0.57797891 0.17577889 0.01480576] 1 1.0 1.0\n",
      "123 [0.01558035 0.82060453 0.11577341 0.04804171] 1 1.0 1.0\n",
      "124 [0.00516153 0.20356111 0.7579357  0.03334166] 2 2.0 2.0\n",
      "125 [0.73508728 0.03706058 0.01633925 0.21151289] 0 2.0 0.0\n",
      "126 [0.11948916 0.01572969 0.42162567 0.44315549] 3 3.0 3.0\n",
      "127 [0.58039901 0.059343   0.31248678 0.04777121] 0 0.0 0.0\n",
      "128 [0.92565135 0.03051392 0.02960052 0.01423421] 0 0.0 0.0\n",
      "129 [0.0089386  0.06156335 0.57226088 0.35723717] 2 2.0 2.0\n",
      "130 [0.39616094 0.09462267 0.34474044 0.16447595] 0 2.0 0.0\n",
      "131 [0.00676788 0.53329488 0.27178251 0.18815473] 1 3.0 1.0\n",
      "132 [0.77368905 0.02568777 0.02594697 0.17467621] 0 0.0 0.0\n",
      "133 [0.40570709 0.02401314 0.13754145 0.43273832] 3 2.0 0.0\n",
      "133 ERROR!!!\n",
      "134 [0.01406996 0.60292655 0.28306109 0.0999424 ] 1 1.0 1.0\n",
      "135 [0.44906264 0.08613168 0.32102247 0.14378321] 0 0.0 0.0\n",
      "136 [9.73947958e-01 7.28658186e-04 2.40421537e-02 1.28123013e-03] 0 0.0 0.0\n",
      "137 [0.00812489 0.10935499 0.48577435 0.39674578] 2 3.0 3.0\n",
      "137 ERROR!!!\n",
      "138 [0.00570444 0.00793654 0.27707548 0.70928353] 3 2.0 3.0\n",
      "139 [0.14919933 0.06013561 0.28208154 0.50858352] 3 2.0 3.0\n",
      "140 [0.027      0.11575549 0.42756849 0.42967602] 3 2.0 3.0\n",
      "141 [0.92193908 0.00225028 0.05794326 0.01786738] 0 0.0 0.0\n",
      "142 [0.75867871 0.04335743 0.15259336 0.0453705 ] 0 0.0 0.0\n",
      "143 [1.19451616e-02 9.74836367e-01 1.24859707e-02 7.32500489e-04] 1 1.0 1.0\n",
      "144 [0.00421812 0.23692023 0.55195825 0.2069034 ] 2 2.0 2.0\n",
      "145 [0.0283748  0.57880581 0.16549857 0.22732082] 1 1.0 1.0\n",
      "146 [0.02519142 0.74530505 0.21675485 0.01274869] 1 1.0 1.0\n",
      "147 [0.80812595 0.01326832 0.08627083 0.09233489] 0 0.0 0.0\n",
      "148 [0.01512114 0.69070621 0.23779284 0.05637981] 1 1.0 1.0\n",
      "149 [0.32717981 0.11353292 0.45146098 0.10782629] 2 0.0 2.0\n",
      "150 [0.0059989  0.01319798 0.36824399 0.61255914] 3 3.0 3.0\n",
      "151 [0.00384528 0.95259937 0.03616757 0.00738778] 1 2.0 1.0\n",
      "152 [0.15330508 0.02224597 0.46901775 0.3554312 ] 2 3.0 2.0\n",
      "153 [0.85908377 0.05156377 0.03855137 0.05080109] 0 0.0 0.0\n",
      "154 [9.64836460e-01 5.59590336e-04 1.31376613e-02 2.14662883e-02] 0 0.0 0.0\n",
      "155 [0.9036805  0.00135087 0.05742442 0.03754422] 0 0.0 0.0\n",
      "156 [0.80034083 0.02348334 0.13656609 0.03960973] 0 0.0 0.0\n",
      "157 [0.32170106 0.00901351 0.29254826 0.37673717] 3 0.0 0.0\n",
      "157 ERROR!!!\n",
      "158 [0.94986438 0.00214175 0.02688911 0.02110475] 0 0.0 0.0\n",
      "159 [9.61968264e-01 3.99544131e-04 1.02085741e-02 2.74236173e-02] 0 0.0 0.0\n",
      "160 [0.00640369 0.23753399 0.43989659 0.31616573] 2 2.0 2.0\n",
      "44\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "WA=0\n",
    "WAP=0\n",
    "for i in range(161):\n",
    "    print(i,X_hot[i],np.argmax(X_hot[i]),YY[i]-1,PRED[i]-1)\n",
    "    if (np.argmax(X_hot[i])!=YY[i]-1):\n",
    "        WA=WA+1\n",
    "    if (PRED[i]!=YY[i]):\n",
    "        WAP=WAP+1\n",
    "    if (np.argmax(X_hot[i])!=PRED[i]-1):\n",
    "        print(i,\"ERROR!!!\")\n",
    "print(WA)\n",
    "print(WAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "48db31ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0843682369424587\n",
      "0.4988968749080183\n",
      "0.20017117465627515\n",
      "0.27061603323269756\n",
      "2.42536827655853\n",
      "1.516632697669617\n",
      "1.7311313654746274\n",
      "0.7892139519011877\n",
      "0.2526074561157646\n",
      "0.5467200209623586\n",
      "0.6815701070684675\n",
      "1.0660125153506004\n",
      "0.51959577048864\n",
      "1.5379456756194394\n",
      "0.32537800653883897\n",
      "0.14334794188620084\n",
      "1.1561871521683846\n",
      "0.6923729620642112\n",
      "0.8437645283052116\n",
      "0.5588157384661248\n",
      "0.32441202394029384\n",
      "0.6716932778378704\n",
      "0.05846464106868964\n",
      "1.062305989620069\n",
      "0.6135353331547985\n",
      "0.04486367102997579\n",
      "1.2120636610302742\n",
      "2.2787299475975606\n",
      "0.6678214459416986\n",
      "0.5995428239477376\n",
      "0.09531771978207197\n",
      "0.5721908134049483\n",
      "0.5340655302714735\n",
      "3.718981250533515\n",
      "0.26175928764875905\n",
      "0.763594604303695\n",
      "0.5843276077077479\n",
      "0.37671976130203294\n",
      "0.36811563016548754\n",
      "0.4674913462871989\n",
      "0.950213037156102\n",
      "0.9908599274277559\n",
      "0.4728482138326848\n",
      "0.946471742644653\n",
      "1.075395976698578\n",
      "0.6412756780178545\n",
      "0.5307302905182271\n",
      "0.3380971563132287\n",
      "0.6124261477026675\n",
      "0.9085825535789998\n",
      "0.7732230897240316\n",
      "0.06709322048964353\n",
      "0.8445334295575159\n",
      "0.7801630567082535\n",
      "0.029966693061955794\n",
      "0.04745651619898861\n",
      "0.8271349003529441\n",
      "1.8341634423389723\n",
      "1.95103295924039\n",
      "0.05151835893781942\n",
      "1.2211209637208298\n",
      "0.44510513725890294\n",
      "2.155644629761274\n",
      "0.5308921283669975\n",
      "0.7028837943317411\n",
      "0.7695223107124893\n",
      "0.2036081221843022\n",
      "0.16694971231150832\n",
      "1.2734515753274052\n",
      "0.37819704662106907\n",
      "0.31177571333387155\n",
      "0.4920516098092584\n",
      "0.6883083602783471\n",
      "0.6351967554681088\n",
      "0.756849315378009\n",
      "0.9005590469512291\n",
      "1.0003860899213919\n",
      "0.4297396902549473\n",
      "1.844780088110816\n",
      "0.41040000869634424\n",
      "0.624260341672167\n",
      "1.223539745441872\n",
      "0.2741670637587181\n",
      "0.554690134970994\n",
      "1.2082102153651575\n",
      "0.45172264412656626\n",
      "2.690179340367577\n",
      "0.4750272137757339\n",
      "0.729013727775274\n",
      "0.6363253061494158\n",
      "0.2793641062941651\n",
      "1.7342050911341038\n",
      "0.6252403572443379\n",
      "0.30889676881010253\n",
      "1.000452983136961\n",
      "0.7243972975116831\n",
      "1.5111330676386734\n",
      "0.7928405981602991\n",
      "0.9377240217324204\n",
      "0.9615266556852529\n",
      "0.4418345220665866\n",
      "1.5070995144586503\n",
      "0.48816029773180336\n",
      "0.9264310838394173\n",
      "0.4000931504931502\n",
      "1.8883143790844146\n",
      "0.6785025637738809\n",
      "0.5589932294787078\n",
      "0.31060434369293954\n",
      "0.8791096249883327\n",
      "0.380194088206585\n",
      "0.5257734390990694\n",
      "0.8713065511043457\n",
      "0.3792081896445478\n",
      "0.22403874965743997\n",
      "0.7519174146509732\n",
      "3.1594623337214087\n",
      "0.37366523822766495\n",
      "0.45901755912445613\n",
      "0.8725218416184722\n",
      "0.8697709211357292\n",
      "0.7347060105541199\n",
      "0.5482177190278269\n",
      "0.19771386148331252\n",
      "0.2771565907206132\n",
      "4.1141787714803515\n",
      "0.8138343608660223\n",
      "0.544039294130993\n",
      "0.07725751999060117\n",
      "0.5581601281568682\n",
      "1.0649632151727164\n",
      "1.6704901044665248\n",
      "0.2565851005259907\n",
      "1.9838292449555428\n",
      "0.505959735240245\n",
      "0.800592671560225\n",
      "0.026397305364499957\n",
      "0.9244593112191817\n",
      "1.2834649546517451\n",
      "1.26555874721917\n",
      "0.8496405556875312\n",
      "0.08127602083539923\n",
      "0.27617676564356225\n",
      "0.025485547904631756\n",
      "0.5942826926598124\n",
      "0.5467880691336204\n",
      "0.2939615526784283\n",
      "0.21303722471047176\n",
      "0.37004056278315683\n",
      "1.117245075584557\n",
      "0.49010962754486626\n",
      "3.319589591667246\n",
      "1.0344232893314416\n",
      "0.15188872789251134\n",
      "0.03579655972443932\n",
      "0.10127930504808308\n",
      "0.22271747358998586\n",
      "1.1341322423381293\n",
      "0.05143595542863177\n",
      "0.03877371400387285\n",
      "0.8212153817554312\n",
      "[0.0843682369424587, 0.4988968749080183, 0.20017117465627515, 0.27061603323269756, 2.42536827655853, 1.516632697669617, 1.7311313654746274, 0.7892139519011877, 0.2526074561157646, 0.5467200209623586, 0.6815701070684675, 1.0660125153506004, 0.51959577048864, 1.5379456756194394, 0.32537800653883897, 0.14334794188620084, 1.1561871521683846, 0.6923729620642112, 0.8437645283052116, 0.5588157384661248, 0.32441202394029384, 0.6716932778378704, 0.05846464106868964, 1.062305989620069, 0.6135353331547985, 0.04486367102997579, 1.2120636610302742, 2.2787299475975606, 0.6678214459416986, 0.5995428239477376, 0.09531771978207197, 0.5721908134049483, 0.5340655302714735, 3.718981250533515, 0.26175928764875905, 0.763594604303695, 0.5843276077077479, 0.37671976130203294, 0.36811563016548754, 0.4674913462871989, 0.950213037156102, 0.9908599274277559, 0.4728482138326848, 0.946471742644653, 1.075395976698578, 0.6412756780178545, 0.5307302905182271, 0.3380971563132287, 0.6124261477026675, 0.9085825535789998, 0.7732230897240316, 0.06709322048964353, 0.8445334295575159, 0.7801630567082535, 0.029966693061955794, 0.04745651619898861, 0.8271349003529441, 1.8341634423389723, 1.95103295924039, 0.05151835893781942, 1.2211209637208298, 0.44510513725890294, 2.155644629761274, 0.5308921283669975, 0.7028837943317411, 0.7695223107124893, 0.2036081221843022, 0.16694971231150832, 1.2734515753274052, 0.37819704662106907, 0.31177571333387155, 0.4920516098092584, 0.6883083602783471, 0.6351967554681088, 0.756849315378009, 0.9005590469512291, 1.0003860899213919, 0.4297396902549473, 1.844780088110816, 0.41040000869634424, 0.624260341672167, 1.223539745441872, 0.2741670637587181, 0.554690134970994, 1.2082102153651575, 0.45172264412656626, 2.690179340367577, 0.4750272137757339, 0.729013727775274, 0.6363253061494158, 0.2793641062941651, 1.7342050911341038, 0.6252403572443379, 0.30889676881010253, 1.000452983136961, 0.7243972975116831, 1.5111330676386734, 0.7928405981602991, 0.9377240217324204, 0.9615266556852529, 0.4418345220665866, 1.5070995144586503, 0.48816029773180336, 0.9264310838394173, 0.4000931504931502, 1.8883143790844146, 0.6785025637738809, 0.5589932294787078, 0.31060434369293954, 0.8791096249883327, 0.380194088206585, 0.5257734390990694, 0.8713065511043457, 0.3792081896445478, 0.22403874965743997, 0.7519174146509732, 3.1594623337214087, 0.37366523822766495, 0.45901755912445613, 0.8725218416184722, 0.8697709211357292, 0.7347060105541199, 0.5482177190278269, 0.19771386148331252, 0.2771565907206132, 4.1141787714803515, 0.8138343608660223, 0.544039294130993, 0.07725751999060117, 0.5581601281568682, 1.0649632151727164, 1.6704901044665248, 0.2565851005259907, 1.9838292449555428, 0.505959735240245, 0.800592671560225, 0.026397305364499957, 0.9244593112191817, 1.2834649546517451, 1.26555874721917, 0.8496405556875312, 0.08127602083539923, 0.27617676564356225, 0.025485547904631756, 0.5942826926598124, 0.5467880691336204, 0.2939615526784283, 0.21303722471047176, 0.37004056278315683, 1.117245075584557, 0.49010962754486626, 3.319589591667246, 1.0344232893314416, 0.15188872789251134, 0.03579655972443932, 0.10127930504808308, 0.22271747358998586, 1.1341322423381293, 0.05143595542863177, 0.03877371400387285, 0.8212153817554312]\n",
      "[[0.0843682369424587, 0.4988968749080183, 0.20017117465627515, 0.27061603323269756, 2.42536827655853, 1.516632697669617, 1.7311313654746274, 0.7892139519011877, 0.2526074561157646, 0.5467200209623586, 0.6815701070684675, 1.0660125153506004, 0.51959577048864, 1.5379456756194394, 0.32537800653883897, 0.14334794188620084, 1.1561871521683846, 0.6923729620642112, 0.8437645283052116, 0.5588157384661248, 0.32441202394029384, 0.6716932778378704, 0.05846464106868964, 1.062305989620069, 0.6135353331547985, 0.04486367102997579, 1.2120636610302742, 2.2787299475975606, 0.6678214459416986, 0.5995428239477376, 0.09531771978207197, 0.5721908134049483, 0.5340655302714735, 3.718981250533515, 0.26175928764875905, 0.763594604303695, 0.5843276077077479, 0.37671976130203294, 0.36811563016548754, 0.4674913462871989, 0.950213037156102, 0.9908599274277559, 0.4728482138326848, 0.946471742644653, 1.075395976698578, 0.6412756780178545, 0.5307302905182271, 0.3380971563132287, 0.6124261477026675, 0.9085825535789998, 0.7732230897240316, 0.06709322048964353, 0.8445334295575159, 0.7801630567082535, 0.029966693061955794, 0.04745651619898861, 0.8271349003529441, 1.8341634423389723, 1.95103295924039, 0.05151835893781942, 1.2211209637208298, 0.44510513725890294, 2.155644629761274, 0.5308921283669975, 0.7028837943317411, 0.7695223107124893, 0.2036081221843022, 0.16694971231150832, 1.2734515753274052, 0.37819704662106907, 0.31177571333387155, 0.4920516098092584, 0.6883083602783471, 0.6351967554681088, 0.756849315378009, 0.9005590469512291, 1.0003860899213919, 0.4297396902549473, 1.844780088110816, 0.41040000869634424, 0.624260341672167, 1.223539745441872, 0.2741670637587181, 0.554690134970994, 1.2082102153651575, 0.45172264412656626, 2.690179340367577, 0.4750272137757339, 0.729013727775274, 0.6363253061494158, 0.2793641062941651, 1.7342050911341038, 0.6252403572443379, 0.30889676881010253, 1.000452983136961, 0.7243972975116831, 1.5111330676386734, 0.7928405981602991, 0.9377240217324204, 0.9615266556852529, 0.4418345220665866, 1.5070995144586503, 0.48816029773180336, 0.9264310838394173, 0.4000931504931502, 1.8883143790844146, 0.6785025637738809, 0.5589932294787078, 0.31060434369293954, 0.8791096249883327, 0.380194088206585, 0.5257734390990694, 0.8713065511043457, 0.3792081896445478, 0.22403874965743997, 0.7519174146509732, 3.1594623337214087, 0.37366523822766495, 0.45901755912445613, 0.8725218416184722, 0.8697709211357292, 0.7347060105541199, 0.5482177190278269, 0.19771386148331252, 0.2771565907206132, 4.1141787714803515, 0.8138343608660223, 0.544039294130993, 0.07725751999060117, 0.5581601281568682, 1.0649632151727164, 1.6704901044665248, 0.2565851005259907, 1.9838292449555428, 0.505959735240245, 0.800592671560225, 0.026397305364499957, 0.9244593112191817, 1.2834649546517451, 1.26555874721917, 0.8496405556875312, 0.08127602083539923, 0.27617676564356225, 0.025485547904631756, 0.5942826926598124, 0.5467880691336204, 0.2939615526784283, 0.21303722471047176, 0.37004056278315683, 1.117245075584557, 0.49010962754486626, 3.319589591667246, 1.0344232893314416, 0.15188872789251134, 0.03579655972443932, 0.10127930504808308, 0.22271747358998586, 1.1341322423381293, 0.05143595542863177, 0.03877371400387285, 0.8212153817554312]]\n"
     ]
    }
   ],
   "source": [
    "Y_hot = np.array([[0 for i in range(4)] for j in range(161)])\n",
    "for i in range(161):\n",
    "    S = int(YY[i] - 1)\n",
    "    Y_hot[i][S] = 1\n",
    "Y_loss = [0 for i in range(161)]\n",
    "for i in range(161):\n",
    "    Y_loss[i] = categorical_cross_entropy(Y_hot[i], X_hot[i])\n",
    "    print(Y_loss[i])\n",
    "lists = [Y_loss]\n",
    "print(Y_loss)\n",
    "print(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f611386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy(y_true, y_pred):\n",
    "    # 避免出现概率为0的情况，加上一个小的偏移量\n",
    "    epsilon = 1e-7\n",
    "    # 计算交叉熵损失\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d26244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a3f50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
